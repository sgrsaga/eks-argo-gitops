name: 'EKS-GitOps-Cluster-Pipeline'

# When the GitHub Action workflow should trigger
on:
  push:
    branches: ["development", "staging", "production" ]
  pull_request:
    branches: [ "staging", "production" ]

# Define ENV variables for the project
env:
  CLUSTER_NAME: EKS-GitOps-Cluster
  REGION: ap-south-1
  SCRIPTS_DIR: ~/work/eks-argo-gitops/eks-argo-gitops/terraform/environment/scripts/

permissions:
  id-token: write # This is required for requesting the JWT
  contents: read

jobs:
  EKS-GitOps-Cluster:
    name: 'Terraform'
    runs-on: ubuntu-latest
    environment: development

    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash

    steps:
    # Checkout the repository to the GitHub Actions runner in Pirivate repo or external repo if required
    # - name: Checkout
    #   uses: actions/checkout@v3
    #   with:
    #     ref : main
    #   env:
    #     GITHUB_TOKEN: $${{ secrets.GIT_REPO_ACCESSS_TOKEN }}

    # Checkout the repository to the GitHub Actions runner in Public repo
    - name: Checkout
      uses: actions/checkout@v3

    # Set AWS Credentials - Use pre-configured OIDC to communicate to AWS
    - name: Configure AWS credentials with OIDC and Role
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.OIDC_ARN }}
        aws-region: ap-south-1

    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.
    - name: Terraform Init
      run: |
        pwd
        echo "${GITHUB_REF##*/}"
        echo "${GITHUB_ENV}"
        cd terraform/environment/${GITHUB_REF##*/}/
        pwd
        terraform init

    # Checks that all Terraform configuration files adhere to a canonical format
    - name: Terraform Format
      run: |
        cd terraform/environment/${GITHUB_REF##*/}/
        terraform fmt

    # # Configure environment - Check EKS Sluster status, Configure kubectl
    # - name: AWS EKS Cluster status check and kubectl configure
    #   id: eks_status
    #   run: |
    #     cd terraform/environment/${GITHUB_REF##*/}/
    #     aws --version
    #     aws s3 ls
    #     echo '#Install kubectl'
    #     curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    #     sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
    #     kubectl version --client
    #     cd ../scripts/
    #     chmod +x check_eks_status.sh
    #     ./check_eks_status.sh

    # # Generates an execution plan for Terraform
    # - name: Terraform Plan
    #   run: |
    #     cd terraform/environment/${GITHUB_REF##*/}/
    #     # Run Terraform Plan
    #     terraform plan -input=false


    # # Apply targetted module. We have designed the modules in a way that we can bring up them with some level of independence
    # - name: Terraform Apply for Network, EKS and EKS Access modules
    #   if: github.ref == 'refs/heads/development' && github.event_name == 'push'
    #   run: |
    #     cd terraform/environment/${GITHUB_REF##*/}/
    #     terraform apply -target=module.main_network -target=module.eks_gitops_cluster -target=module.eks_access --auto-approve -input=false

    # # # Apply HELM Module
    # - name: Terraform Apply HELM Module
    #   if: github.ref == 'refs/heads/development' && github.event_name == 'push'
    #   run: |
    #     cd terraform/environment/scripts/
    #     ./check_eks_status.sh
    #     cd ../${GITHUB_REF##*/}/
    #     echo "Apply helm module"
    #     terraform apply -target=module.helm_repos --auto-approve -input=false

    # # # Destroy the whole infrastructure
    - name: Terraform Destroy
      if: github.ref == 'refs/heads/development' && github.event_name == 'push'
      run: |
        cd cd terraform/environment/scripts/
        chmod +x check_eks_status.sh
        ./check_eks_status.sh
        cd terraform/environment/${GITHUB_REF##*/}/
        terraform destroy --auto-approve -input=false

    # # Apply Argocd, Prometheus, Grafana and Loki Ingress
    # - name: Apply Ingress with Kustomize
    #   run: |
    #     cd terraform/environment/scripts/
    #     chmod +x check_eks_status.sh
    #     ./check_eks_status.sh
    #     cd ../../../kubernetes/overlays/development/ingress
    #     pwd
    #     echo 'Apply Ingress with Kustomize'
    #     kubectl kustomize .
    #     kubectl apply -k .
    #     kubectl get po -n dev
